{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1293a87b",
   "metadata": {},
   "source": [
    "# Preparing data for passim\n",
    "\n",
    "These scripts are used to prepare the data for the passim alignment process.\n",
    "  \n",
    "The input datas are the xml altos from eScriptorium containing the OCR text,\n",
    "and the digital editions from Sefaria (cleaned and concatenated with this pipeline:\n",
    "https://github.com/Freymat/from_Sefaria_to_Passim).\n",
    "\n",
    "The output file, that will be processed with passim is a jsonl file, each line of which is a dictionary containing\n",
    "either:\n",
    "- the content of an OCR textblock. These textblocks are constituted by the concatenation of the text of the OCR lines, from a part region.\n",
    "  The content of the OCR lines is retrieved from the xmls alto files from eScriptorium.\n",
    "- the text of a digital edition (Ground Truth), concatenated in one line. Those texts are retrieved from Sefaria, cleanded and concatenated.\n",
    "\n",
    "Passim will then align the OCR textblocks with the Ground Truth texts, and will output a jsonl file containing the alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabdbbd",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d715557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switching to  msIA\n"
     ]
    }
   ],
   "source": [
    "import json # To work with JSON data\n",
    "import jsonlines # To write data in JSON Lines format\n",
    "import os # To interact with the operating system\n",
    "import glob # To search for files in a directory\n",
    "from xml.etree import ElementTree # To parse XML files\n",
    "import subprocess # To run Passim\n",
    "from pprint import pprint\n",
    "\n",
    "# Import functions for eScriptorium's API\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f3153",
   "metadata": {},
   "source": [
    "### Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763b7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth files\n",
    "GT_texts_path = \"digital_editions\"\n",
    "\n",
    "# xmls files from eScriptorium (OCR results) /path\n",
    "xmls_directory_path = \"xmls_from_eSc\"\n",
    "\n",
    "# path to the dictionary that will contain the extracted textblocks from OCR\n",
    "ocr_lines_dict_path = \"ocr_lines_dict/ocr_lines_dict.json\"\n",
    "\n",
    "# path for the JSON file that will be used as input for Passim\n",
    "input_passim_path = \"json_for_passim/passim_input.json\" # path for the output JSON file. This file will be used as input for Passim.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36101843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list where output datas for Passim will be stored\n",
    "def initialize_passim_input():\n",
    "    global passim_input\n",
    "    passim_input = []\n",
    "    return passim_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b7e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_passim_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b628234f",
   "metadata": {},
   "source": [
    "### Build the content of OCR textblocks and prepare them for Passim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7cb6f",
   "metadata": {},
   "source": [
    "#### Loop through XML files from eSC, and extract TextLine elements text and ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0d5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_ocr_textblocks(xmls_directory_path, ocr_lines_dict_path):\n",
    "    \"\"\" This script reads the XML alto files produced by eScriptorium,\n",
    "    extracts and concatenate the text of the ocr lines from each TextBlock elements.\n",
    "\n",
    "    The result is a list of dictionaries, one per alto file.\n",
    "    Each dictionnary contains the list of the text blocks in the file. Each element of this list is a dictionary containing:\n",
    "    - the concatenated text of the lines in the TextBlock element,\n",
    "    - the ID of the TextBlock element,\n",
    "    - the IDs of the TextLine elements in the text block, and the starting position of each line in the concatenated text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize list to store parts\n",
    "    parts = []\n",
    "\n",
    "    # Loop through all XML files in the directory\n",
    "    for filename in glob.glob(os.path.join(xmls_directory_path, \"*.xml\")):\n",
    "        # Obtenir le nom de chaque fichier\n",
    "        basename = os.path.splitext(os.path.basename(filename))[0]\n",
    "\n",
    "        # Initialize list to store text blocks\n",
    "        blocks = []\n",
    "\n",
    "        # Parse the XML file\n",
    "        tree = ElementTree.parse(filename)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Loop through all TextBlock elements in the XML file\n",
    "        for text_block in root.iter(\"{http://www.loc.gov/standards/alto/ns-v4#}TextBlock\"):\n",
    "            # Obtenir l'ID de l'attribut ID de l'élément TextBlock\n",
    "            text_block_id = text_block.get(\"ID\")\n",
    "\n",
    "            lines = []\n",
    "            continuous_text = \"\"\n",
    "            char_count = 0  # Initial position for continuous text\n",
    "\n",
    "            # Loop through all TextLine elements in the TextBlock element\n",
    "            for text_line in text_block.iter(\"{http://www.loc.gov/standards/alto/ns-v4#}TextLine\"):\n",
    "                text = text_line.find(\"{http://www.loc.gov/standards/alto/ns-v4#}String\").get(\"CONTENT\").strip()\n",
    "\n",
    "                line_dict = {\n",
    "                    \"line_id\": text_line.get(\"ID\"),\n",
    "                    \"start\": char_count,  # Start position of line in continuous text\n",
    "                    \"end\" : char_count + len(text) -1,\n",
    "                    \"length\": len(text),\n",
    "                    \"text\": text\n",
    "                }\n",
    "                separator = \"\\n\"\n",
    "                continuous_text += (text + separator)\n",
    "                char_count += len(text + separator)  # Updates the starting position for the next line.\n",
    "\n",
    "                lines.append(line_dict)\n",
    "\n",
    "            # Add text block to block list\n",
    "            blocks.append({\n",
    "                \"ocr_block_text\": continuous_text.strip(),  # Removes superfluous spaces at the beginning and end\n",
    "                \"text_block_id\": text_block_id,\n",
    "                \"ocr_lines_in_block\": len(lines),  # Number of lines in the block\n",
    "                \"ocr_lines\": lines,\n",
    "                \"series\": 'OCR'  # Distinguishes OCR from control\n",
    "            })\n",
    "\n",
    "        # Ajouter un document avec ses blocs correspondants à la liste des parties\n",
    "\n",
    "        parts.append({\n",
    "            \"filename\": basename,\n",
    "            \"ocr_lines_in_part\": sum([block[\"ocr_lines_in_block\"] for block in blocks]),\n",
    "            \"ocr_blocks\": blocks,\n",
    "            # total number of ocr lines in the xml part, for the selected text regions\n",
    "            \"ocr_lines_in_part\": sum([block[\"ocr_lines_in_block\"] for block in blocks])\n",
    "        })\n",
    "\n",
    "    # Save the 'parts' dictionnary to a JSON file named lines_dict\n",
    "    if not os.path.exists(\"ocr_lines_dict\"):\n",
    "        os.makedirs(\"ocr_lines_dict\")\n",
    "\n",
    "    # Save the 'parts' dictionnary to a JSON file\n",
    "    with open(ocr_lines_dict_path, \"w\", encoding=\"utf-8\") as file_handler:\n",
    "        json.dump(parts, file_handler, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # pprint(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e49c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_ocr_textblocks(xmls_directory_path, ocr_lines_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6234d",
   "metadata": {},
   "source": [
    "#### Build the input data for passim, from the OCR line dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de87fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_OCR_textblocks_to_passim_input(ocr_lines_dict_path):\n",
    "    '''\n",
    "    Read the JSON file containing the OCR textblocks and build the input for Passim.\n",
    "    GT texts still need to be added to the input. \n",
    "    '''\n",
    "    # open the dictionnary\n",
    "    with open(ocr_lines_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        parts = json.load(f)        \n",
    "    \n",
    "    for part in parts:\n",
    "        for block in part[\"ocr_blocks\"]:\n",
    "            text_block_id = block[\"text_block_id\"]\n",
    "            text_block_text = block[\"ocr_block_text\"]\n",
    "            filename = part[\"filename\"]\n",
    "            passim_input.append({\"id\": text_block_id +'_' + filename, \"series\": 'OCR',\"ref\": '0', \"text\": text_block_text})\n",
    "            # print(text_block_id, filename)\n",
    "    # print(passim_input)\n",
    "    return passim_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8d6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_OCR_textblocks_to_passim_input(ocr_lines_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48c8a5",
   "metadata": {},
   "source": [
    "### Build the GT text datas for Passim\n",
    "Add every txt file in the GT directory to the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a948e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_GT_texts_to_passim_input(GT_texts_path):\n",
    "    '''\n",
    "    Add every digital witness text to the passim_input list \n",
    "    '''\n",
    "    for root, dirs, files in os.walk(GT_texts_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                text_file = os.path.join(root, file)\n",
    "                with open(text_file, \"r\", encoding=\"utf-8\") as file_handler:\n",
    "                    text = file_handler.read()\n",
    "                    filename = os.path.basename(text_file)\n",
    "                    passim_input.append({\"id\": filename, \"series\": 'GT', \"ref\": '1', \"text\": text})\n",
    "                    # print(f\"Added to output: {filename}\")\n",
    "       # print(passim_input)\n",
    "    return passim_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "113cf52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(passim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b86fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_GT_texts_to_passim_input(GT_texts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc710f8",
   "metadata": {},
   "source": [
    "### Writing Data to JSONLines File in Compact Format without ASCII Encoding, for Passim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1438a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_passim_input_to_json(input_passim_path, passim_input):\n",
    "    ''' \n",
    "    Write Data to JSONLines File in compact format without ASCII Encoding, for Passim\n",
    "    '''\n",
    "    # Open the output file in write mode\n",
    "    with open(input_passim_path, \"w\", encoding=\"utf-8\") as file_handler:\n",
    "        # Create a jsonlines writer object that writes to the output file\n",
    "        writer = jsonlines.Writer(file_handler)\n",
    "        # Loop through each item in the output_data list\n",
    "        for item in passim_input:\n",
    "            # Write the current item to the output file using the jsonlines writer\n",
    "            writer.write(item)\n",
    "    print(f\"input file for passim created: {input_passim_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb44ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_passim_input_to_json(input_passim_path, passim_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0ef44",
   "metadata": {},
   "source": [
    "### Build the input for Passim - global function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b71f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_passim_input(xmls_directory_path, ocr_lines_dict_path, GT_texts_path, input_passim_path):\n",
    "    '''\n",
    "    Build the input for Passim from the OCR textblocks and the GT texts.\n",
    "    Parameters:\n",
    "    - xmls_directory_path: path to the directory containing the XML alto files imported from eScriptorium, containing the OCR results.\n",
    "    - ocr_lines_dict_path: path to the JSON file that will contain the extracted textblocks from OCR.\n",
    "    - GT_texts_path: path to the directory containing the ground truth texts.\n",
    "    - input_passim_path: path for the output JSON file. This file will be used as input for Passim.\n",
    "    '''\n",
    "    # Initialize the list where output datas for Passim will be stored\n",
    "    initialize_passim_input()\n",
    "    # Extract OCR textblocks from XML alto files imported from eScriptorium\n",
    "    extract_ocr_textblocks(xmls_directory_path, ocr_lines_dict_path)\n",
    "    # Add OCR textblocks to the passim_input list\n",
    "    add_OCR_textblocks_to_passim_input(ocr_lines_dict_path)\n",
    "    # Add GT texts to the passim_input list\n",
    "    add_GT_texts_to_passim_input(GT_texts_path)\n",
    "    # Write the passim_input list to a JSON file\n",
    "    write_passim_input_to_json(input_passim_path, passim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecae170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file for passim created: json_for_passim/passim_input.json\n"
     ]
    }
   ],
   "source": [
    "build_passim_input(xmls_directory_path, ocr_lines_dict_path, GT_texts_path, input_passim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd3a01",
   "metadata": {},
   "source": [
    "# Composing the command to run Passim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6e3f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session interactive HTC:\n",
      "srun -t 12:00:00 -n 60 --mem 128G --pty bash -i\n",
      "Lauch Singularity container\n",
      "singularity shell --bind /sps:/sps --bind /pbs:/pbs --nv /sps/humanum/eScriptorium/share/acdc.sif\n",
      "Passim command line:\n",
      "SPARK_SUBMIT_ARGS='--master local[60] --executor-memory 128G --driver-memory 40G' seriatim --docwise --floating-ngrams --fields ref --filterpairs 'ref = 1 AND ref2 = 0' --all-pairs --complete-lines -n 15 passim_input.json out_n15_docwise\n"
     ]
    }
   ],
   "source": [
    "# Command Line to request an interactive session on HTC\n",
    "# example: % srun -t 0-08:00 -n 4 --mem 2G --pty bash -i\n",
    "t = \"12:00:00\" # Session duration - hours:minutes:seconds\n",
    "n_cores = 60 # number of cpu cores\n",
    "mem = 128 # memory per node, in GB\n",
    "driver_mem = 40 # memory for the driver, in GB\n",
    "\n",
    "\n",
    "command_srun = f\"srun -t {t} -n {n_cores} --mem {mem}G --pty bash -i\"\n",
    "\n",
    "# Command Line to run Passim\n",
    "\n",
    "n = 15 # n-gram order (default: 25) \n",
    "# m = 5 # Minimum number of n-gram matches between document (default: 5)\n",
    "# a = 7 # Minimum length of alignment (default: 50)\n",
    "# g = 25 # Minimum size of gap that separates passages (default: 600)\n",
    "align_mode = 'docwise' # alignment mode\n",
    "\n",
    "input_file = \"passim_input.json\" # input file for Passim\n",
    "output_folder = f\"out_n{n}_{align_mode}\"\n",
    "\n",
    "command_passim = f\"SPARK_SUBMIT_ARGS='--master local[{n_cores}] --executor-memory {mem}G --driver-memory {driver_mem}G' seriatim --{align_mode} --floating-ngrams --fields ref --filterpairs 'ref = 1 AND ref2 = 0' --all-pairs --complete-lines -n {n} {input_file} {output_folder}\"\n",
    "\n",
    "print(f\"Session interactive HTC:\\n{command_srun}\")\n",
    "print(f\"Lauch Singularity container\")\n",
    "print(r\"singularity shell --bind /sps:/sps --bind /pbs:/pbs --nv /sps/humanum/eScriptorium/share/acdc.sif\")\n",
    "print(f\"Passim command line:\\n{command_passim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f7ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /home/matthieu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/matthieu/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e70355d0-3756-4e46-947a-16d0cd92321e;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/matthieu/anaconda3/envs/acdc/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound graphframes#graphframes;0.8.0-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 274ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.0-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e70355d0-3756-4e46-947a-16d0cd92321e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(id='id', text='text', locs='locs', pages='pages', minDF=2, maxDF=100, min_match=5, n=7, floating_ngrams=True, complete_lines=True, gap=600, max_offset=20, beam=20, pcopy=0.8, min_align=50, src_overlap=0.9, dst_overlap=0.5, fields=['ref'], filterpairs='ref = 1 AND ref2 = 0', all_pairs=True, pairwise=False, docwise=True, linewise=False, to_pairs=False, to_extents=False, link_model=None, link_features=None, log_level='WARN', input_format='json', output_format='json', inputPath='json_for_passim/passim_input.json', outputPath='json_from_passim/out_n7_docwise_complete-lines')\n",
      "15:02:27.011 [Thread-5] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=\"SPARK_SUBMIT_ARGS='--master local[4] --executor-memory 6G --driver-memory 4G' seriatim --docwise --floating-ngrams --fields ref --filterpairs 'ref = 1 AND ref2 = 0' --all-pairs --complete-lines -n 7 json_for_passim/passim_input.json json_from_passim/out_n7_docwise_complete-lines\", returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Executing passim locally\n",
    "subprocess.run(command_passim, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
